# Data Processing and CI/CD Project

This project demonstrates a robust data processing pipeline using Python with Pandas, integrated with a GitHub Actions CI/CD workflow for automated execution, linting (Ruff), and publication of results via GitHub Pages.

## Project Structure

The core components of this project include:

*   `data.xlsx`: The raw input data in Excel format.
*   `execute.py`: A Python script that processes `data.xlsx` and generates analytical output.
*   `.github/workflows/ci.yml`: The GitHub Actions workflow definition for automation.
*   `result.json`: The analytical output generated by `execute.py` during CI, published via GitHub Pages (not committed to the repository).

## `execute.py` - Data Processing Script

The `execute.py` script is responsible for reading the `data.xlsx` file, performing data cleaning and aggregation, and outputting the results as a JSON object to standard output.

### Fixes and Enhancements (Python 3.11+, Pandas 2.3+)

The script has been revised to ensure:
*   **Pandas 2.3+ Compatibility**: Utilizes up-to-date Pandas API calls and best practices.
*   **Robust Error Handling**: Includes `try-except` blocks for file operations and data type conversions to gracefully handle malformed data or missing files.
*   **Data Validation**: Ensures that critical columns (e.g., 'Value') are correctly converted to numeric types, dropping invalid entries to prevent processing errors.
*   **Structured JSON Output**: The script now produces a well-structured JSON output containing summary statistics and aggregated data, making it easy to consume by other applications.

**Example `execute.py` Logic:**

```python
import pandas as pd
import json
import os

def process_data(file_path):
    """
    Reads an Excel file, processes it, and returns aggregated data.
    This function has been fixed to ensure compatibility with Pandas 2.3+
    and robust error handling.
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"The file {file_path} was not found.")

    try:
        df = pd.read_excel(file_path, sheet_name='Sheet1')

        # Ensure 'Value' column is numeric, coercing errors to NaN
        df['Value'] = pd.to_numeric(df['Value'], errors='coerce')

        # Drop rows where 'Value' became NaN after conversion
        df.dropna(subset=['Value'], inplace=True)

        if df.empty:
            return {"status": "success", "message": "No valid data to process.", "data": {}}

        # Perform a simple aggregation: sum of 'Value' per 'Category'
        # Using reset_index() to convert the grouped Series back to a DataFrame
        aggregated_data = df.groupby('Category')['Value'].sum().reset_index()

        # Convert to a list of dictionaries for JSON output
        results = aggregated_data.to_dict(orient='records')

        summary_stats = {
            "total_items": len(df),
            "sum_of_values": df['Value'].sum(),
            "average_value": df['Value'].mean(),
            "unique_categories": df['Category'].nunique()
        }

        return {
            "status": "success",
            "message": "Data processed successfully.",
            "summary": summary_stats,
            "category_totals": results
        }

    except Exception as e:
        return {"status": "error", "message": f"An error occurred during data processing: {e}"}

if __name__ == "__main__":
    excel_file = 'data.xlsx'
    output_result = process_data(excel_file)

    # Output to stdout, which will be redirected to result.json by the CI
    print(json.dumps(output_result, indent=2))
```

## `data.xlsx` and `data.csv`

*   `data.xlsx` serves as the primary input for the `execute.py` script.
*   While `execute.py` directly processes `data.xlsx`, `data.csv` can also be generated from `data.xlsx` for alternative use cases or for environments where Excel files are not preferred. This conversion can be done manually using Pandas:
    ```bash
    python -c "import pandas as pd; pd.read_excel('data.xlsx').to_csv('data.csv', index=False)"
    ```
    For this project's CI, `execute.py` is configured to read `data.xlsx` directly.

## GitHub Actions Workflow (`.github/workflows/ci.yml`)

A GitHub Actions workflow is configured to automate the project's tasks on every push to the repository.

### Workflow Details:

*   **Linter (Ruff)**: Ensures code quality and consistency by running `ruff` on all Python files. The results are shown in the CI log.
*   **Script Execution**: Runs `python execute.py` and redirects its output to `result.json`.
*   **GitHub Pages Publication**: The generated `result.json` is then published as a static asset via GitHub Pages, making the analytical results publicly accessible.

**`.github/workflows/ci.yml` content:**

```yaml
name: CI/CD Pipeline

on:
  push:
    branches:
      - main
      - master

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write # To allow checkout and upload artifact
      pages: write    # To deploy to GitHub Pages
      id-token: write # To authenticate with GitHub Pages

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # Ensure compatibility with Python 3.11+

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas openpyxl ruff

      - name: Run Ruff Linter
        run: |
          ruff check .
          ruff format . --check # Check for formatting issues

      - name: Run execute.py and generate result.json
        run: |
          python execute.py > result.json

      - name: Upload result.json as artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'result.json'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
```

## How to Access Results

After a successful push to the `main` or `master` branch, the `result.json` file will be generated and published to GitHub Pages. You can typically find it at:

`https://<YOUR_GITHUB_USERNAME>.github.io/<YOUR_REPOSITORY_NAME>/result.json`

Replace `<YOUR_GITHUB_USERNAME>` and `<YOUR_REPOSITORY_NAME>` with your actual GitHub details.

## Setup and Local Execution

To run the script and checks locally:

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/<YOUR_GITHUB_USERNAME>/<YOUR_REPOSITORY_NAME>.git
    cd <YOUR_REPOSITORY_NAME>
    ```
2.  **Install Python (3.11+) and dependencies:**
    ```bash
    # It's recommended to use a virtual environment
    python3 -m venv .venv
    source .venv/bin/activate # On Windows: .venv\Scripts\activate
    pip install pandas openpyxl ruff
    ```
3.  **Run Ruff:**
    ```bash
    ruff check .
    ruff format . --check
    ```
4.  **Run the data processing script:**
    ```bash
    python execute.py > result.json
    ```
    This will generate `result.json` in your local directory.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
